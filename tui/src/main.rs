use anyhow::Result;
use commands::{CompletionHelper, CompletionResult, Registrable};
use conversation::Conversation;
use crossterm::{
    event::{self, Event, KeyCode, KeyEventKind, KeyModifiers},
    execute,
    terminal::{disable_raw_mode, enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen},
};
use dotenv;
use futures::StreamExt;
use llm::providers::{ClaudeProvider, GeminiProvider, GeneralModelProvider, OllamaProvider, OpenAIProvider};
use llm::ModelProvider;
use ratatui::{
    backend::CrosstermBackend,
    layout::{Constraint, Direction, Layout},
    style::{Color, Modifier, Style},
    text::{Line, Span, Text},
    widgets::{Block, Borders, List, ListItem, Paragraph},
    Frame, Terminal,
};
use std::io;
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex};
use tui_input::backend::crossterm::EventHandler;
use tui_input::Input;

fn get_api_key(key: &str) -> String {
    let home_dir = if let Some(home) = directories::UserDirs::new() {
        home.home_dir().to_path_buf()
    } else {
        panic!("Could not determine home directory");
    };
    let env_path = home_dir.join(".env");
    dotenv::from_path(env_path).ok();
    std::env::var(key).expect(&format!("{} must be set in .env file", key))
}

#[derive(Clone, Debug, PartialEq, Eq)]
#[commands::completable]
enum ModelProviderType {
    /// Local LLM server
    Ollama,
    /// Google's Gemini models
    Gemini,
    /// Anthropic's Claude models
    Claude,
    /// OpenAI GPT models
    OpenAI,
}

impl std::fmt::Display for ModelProviderType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{:?}", self)
    }
}

// FromStr is now generated by #[completable] macro

fn get_model_info(provider_type: &ModelProviderType) -> (&'static str, &'static str) {
    match provider_type {
        ModelProviderType::Ollama => ("gemma3n:latest", "gemma3n:latest"),
        ModelProviderType::Gemini => ("models/gemini-2.5-flash", "gemini-2.5-flash"),
        ModelProviderType::Claude => ("claude-sonnet-4-5-20250929", "claude-sonnet-4-5"),
        ModelProviderType::OpenAI => ("gpt-4o-mini", "gpt-4o-mini"),
    }
}

fn create_provider(provider_type: &ModelProviderType) -> GeneralModelProvider {
    match provider_type {
        ModelProviderType::Ollama => GeneralModelProvider::Ollama(OllamaProvider::default()),
        ModelProviderType::Gemini => {
            GeneralModelProvider::Gemini(GeminiProvider::default(&get_api_key("GEMINI_API_KEY")))
        }
        ModelProviderType::Claude => {
            GeneralModelProvider::Claude(ClaudeProvider::default(&get_api_key("CLAUDE_API_KEY")))
        }
        ModelProviderType::OpenAI => {
            GeneralModelProvider::OpenAI(OpenAIProvider::default(&get_api_key("OPENAI_API_KEY")))
        }
    }
}

enum MessageCommand {
    SendMessage(String),
}

enum MessageResponse {
    Error(String),
    Chunk(String),
    Complete,
}

enum CompletionState {
    Idle,
    Loading,
    Showing { completions: Vec<commands::Completion>, selected: usize },
}

struct App {
    input: Input,
    conversation: Arc<Mutex<Conversation>>,
    current_provider: ModelProviderType,
    model_display_name: &'static str,
    status_message: Option<String>,
    is_streaming: bool,
    thinking_frame: usize,
    message_tx: mpsc::UnboundedSender<MessageCommand>,
    message_rx: mpsc::UnboundedReceiver<MessageResponse>,
    cmd_rx: Option<(mpsc::UnboundedReceiver<MessageCommand>, mpsc::UnboundedSender<MessageResponse>)>,
    cached_history: Vec<llm::api::ChatMessage>,
    current_response: String,
}

/// Wrapper that owns App and CommandRegistry to avoid borrow checker issues
struct AppWithCommands {
    app: App,
    registry: commands::CommandRegistry<App>,
    completion_state: CompletionState,
}

impl App {
    fn new(provider: ModelProviderType, system_message: Option<String>) -> Result<Self> {
        let (model_id, model_display_name) = get_model_info(&provider);
        let provider_instance = create_provider(&provider);
        let model = provider_instance.create_chat_model(model_id).unwrap();

        let conversation = if let Some(sys_msg) = system_message {
            Conversation::with_system_message(model, sys_msg)
        } else {
            Conversation::new(model)
        };

        let conversation = Arc::new(Mutex::new(conversation));

        // Create channels for background message processing
        let (cmd_tx, cmd_rx) = mpsc::unbounded_channel();
        let (resp_tx, resp_rx) = mpsc::unbounded_channel();

        Ok(App {
            input: Input::default(),
            conversation,
            current_provider: provider,
            model_display_name,
            status_message: None,
            is_streaming: false,
            thinking_frame: 0,
            message_tx: cmd_tx,
            message_rx: resp_rx,
            cmd_rx: Some((cmd_rx, resp_tx)),
            cached_history: Vec::new(),
            current_response: String::new(),
        })
    }

    /// Apply a completion value to the input
    fn apply_completion_value(&mut self, value: &str) {
        let current = self.input.value();
        let new_value = if let Some(last_space) = current.rfind(char::is_whitespace) {
            format!("{} {} ", &current[..=last_space].trim(), value)
        } else {
            format!("/{} ", value)
        };
        self.input = Input::from(new_value);
    }

    fn start(&mut self) {
        let conversation = Arc::clone(&self.conversation);
        let (cmd_rx, resp_tx) = self.cmd_rx.take().expect("start() called twice");

        // Spawn background message processor
        tokio::spawn(Self::message_processor(conversation, cmd_rx, resp_tx));
    }
}

impl AppWithCommands {
    fn new(provider: ModelProviderType, system_message: Option<String>) -> Result<Self> {
        let app = App::new(provider, system_message)?;
        let mut registry = commands::CommandRegistry::new();
        App::register(&mut registry);

        Ok(Self {
            app,
            registry,
            completion_state: CompletionState::Idle,
        })
    }

    fn start(&mut self) {
        self.app.start();
    }

    /// Trigger tab completion using the registry
    async fn trigger_completion(&mut self) {
        let input_value = self.app.input.value().to_string();
        self.completion_state = CompletionState::Loading;

        let helper = CompletionHelper::new(&self.registry);
        let result = helper.trigger_completion(&input_value, &self.app).await;

        match result {
            CompletionResult::Completions(completions) => {
                match completions.len() {
                    0 => {
                        self.completion_state = CompletionState::Idle;
                    }
                    1 => {
                        self.completion_state = CompletionState::Idle;
                        self.app.apply_completion_value(&completions[0].value);
                    }
                    _ => {
                        self.completion_state = CompletionState::Showing {
                            completions,
                            selected: 0,
                        };
                    }
                }
            }
            CompletionResult::AutoFilledPrefix { new_input, completions } => {
                self.app.input = Input::from(new_input);
                self.completion_state = CompletionState::Showing {
                    completions,
                    selected: 0,
                };
            }
        }
    }

    /// Handle a command - returns false if should quit
    async fn handle_command(&mut self, input: &str) -> Result<bool> {
        let tokens = commands::TokenStream::new(input.to_string());
        let ctx = commands::ContextMut::new(tokens, &mut self.app);
        let result = self.registry.execute(ctx).await;

        match result {
            Ok(commands::CommandResult::Success(msg)) => {
                if !msg.is_empty() {
                    self.app.status_message = Some(msg);
                }
                Ok(true)
            }
            Ok(commands::CommandResult::Exit) => Ok(false),
            Err(e) => {
                self.app.status_message = Some(format!("Error: {}", e));
                Ok(true)
            }
        }
    }

    /// Handle a key event - returns false if should quit
    async fn handle_key_event(&mut self, key: crossterm::event::KeyEvent) -> Result<bool> {
        if key.kind != KeyEventKind::Press {
            return Ok(true);
        }

        match (key.code, key.modifiers) {
            (KeyCode::Char('c'), KeyModifiers::CONTROL) | (KeyCode::Char('d'), KeyModifiers::CONTROL) => {
                Ok(false)
            }
            (KeyCode::Tab, _) => {
                if matches!(self.completion_state, CompletionState::Showing { .. }) {
                    self.next_completion();
                } else {
                    self.trigger_completion().await;
                }
                Ok(true)
            }
            (KeyCode::Down, _) => {
                self.handle_down();
                Ok(true)
            }
            (KeyCode::Up, _) => {
                self.handle_up();
                Ok(true)
            }
            (KeyCode::Esc, _) => {
                self.cancel_completion();
                Ok(true)
            }
            (KeyCode::Enter, _) => {
                if let Some(input_text) = self.handle_enter() {
                    self.app.status_message = None;

                    if !input_text.is_empty() {
                        if input_text.starts_with('/') {
                            self.handle_command(&input_text).await
                        } else {
                            self.app.queue_message(input_text);
                            Ok(true)
                        }
                    } else {
                        Ok(true)
                    }
                } else {
                    Ok(true)
                }
            }
            _ => {
                self.cancel_completion();
                self.app.input.handle_event(&Event::Key(key));
                Ok(true)
            }
        }
    }

    /// Select next completion
    fn next_completion(&mut self) {
        if let CompletionState::Showing { ref mut selected, ref completions } = self.completion_state {
            *selected = (*selected + 1) % completions.len();
        }
    }

    /// Select previous completion
    fn prev_completion(&mut self) {
        if let CompletionState::Showing { ref mut selected, ref completions } = self.completion_state {
            *selected = if *selected == 0 {
                completions.len() - 1
            } else {
                *selected - 1
            };
        }
    }

    /// Handle down arrow
    fn handle_down(&mut self) {
        if matches!(self.completion_state, CompletionState::Showing { .. }) {
            self.next_completion();
        } else {
            self.app.input.handle_event(&Event::Key(crossterm::event::KeyEvent::new(
                crossterm::event::KeyCode::Down,
                crossterm::event::KeyModifiers::empty(),
            )));
        }
    }

    /// Handle up arrow
    fn handle_up(&mut self) {
        if matches!(self.completion_state, CompletionState::Showing { .. }) {
            self.prev_completion();
        } else {
            self.app.input.handle_event(&Event::Key(crossterm::event::KeyEvent::new(
                crossterm::event::KeyCode::Up,
                crossterm::event::KeyModifiers::empty(),
            )));
        }
    }

    /// Handle enter key
    fn handle_enter(&mut self) -> Option<String> {
        if let CompletionState::Showing { selected, ref completions } = self.completion_state {
            if let Some(completion) = completions.get(selected) {
                let value = completion.value.clone();
                self.completion_state = CompletionState::Idle;
                self.app.apply_completion_value(&value);
            }
            return None;
        }

        // Not showing completions - return input for processing
        let input_text = self.app.input.value().to_string();
        self.app.input.reset();
        Some(input_text)
    }

    /// Cancel completion
    fn cancel_completion(&mut self) {
        self.completion_state = CompletionState::Idle;
    }
}

// New command system commands
#[commands::commandable]
impl App {
    #[command(name = "help", help = "Show available commands")]
    async fn cmd_help(&mut self) -> Result<String, anyhow::Error> {
        Ok("Available commands:\n  /help - Show this help\n  /clear - Clear conversation\n  /model <provider> - Switch model provider\n  /quit - Exit".to_string())
    }

    #[command(name = "clear", help = "Clear conversation history")]
    async fn cmd_clear(&mut self) -> Result<String, anyhow::Error> {
        if let Ok(mut conv) = self.conversation.try_lock() {
            conv.clear();
            self.cached_history.clear();
            Ok("Conversation cleared".to_string())
        } else {
            Err(anyhow::anyhow!("Cannot clear conversation while streaming"))
        }
    }

    #[command(name = "quit", help = "Exit the application")]
    async fn cmd_quit(&mut self) -> Result<(), anyhow::Error> {
        // Returning Ok(()) will be converted to CommandResult::Success("")
        // The TUI will check for Exit variant separately
        Ok(())
    }

    #[command(name = "model", help = "Switch model provider and optionally select model")]
    async fn cmd_model(
        &mut self,
        provider: ModelProviderType,
        model_name: Option<String>
    ) -> Result<String, anyhow::Error> {
        let (default_model_id, model_display_name) = get_model_info(&provider);
        let provider_instance = create_provider(&provider);

        let model_id = model_name.as_deref().unwrap_or(default_model_id);
        let model = provider_instance.create_chat_model(model_id)
            .ok_or_else(|| anyhow::anyhow!("Model '{}' not found for provider {:?}", model_id, provider))?;

        if let Ok(mut conv) = self.conversation.try_lock() {
            conv.set_model(model);
            self.current_provider = provider;
            self.model_display_name = model_display_name;
            Ok(format!("Switched to {} • {}", self.current_provider, model_id))
        } else {
            Err(anyhow::anyhow!("Cannot switch model while streaming"))
        }
    }

    #[completer(arg = "model_name")]
    async fn complete_model_name(
        &self,
        provider: &ModelProviderType,
        partial: &str
    ) -> Result<Vec<commands::Completion>, anyhow::Error> {
        let provider_instance = create_provider(provider);
        let models = provider_instance.list_models().await?;

        // Filter to only text-capable models and match partial
        Ok(models
            .into_iter()
            .filter(|m| {
                // Only include models that support text/chat
                if !m.has_capability(&llm::ModelCapability::Text) {
                    return false;
                }
                // Only include models that can be created
                if provider_instance.create_chat_model(&m.id).is_none() {
                    return false;
                }
                // Match partial input
                m.id.to_lowercase().starts_with(&partial.to_lowercase())
            })
            .map(|m| commands::Completion::simple(m.id))
            .collect())
    }
}

// Old handle_command removed - now using built-in App::handle_command

impl App {
    fn queue_message(&mut self, message: String) {
        // Update cached history with user message immediately
        if let Ok(conv) = self.conversation.try_lock() {
            self.cached_history = conv.history().to_vec();
        }
        self.cached_history.push(llm::api::ChatMessage::user(message.clone().into()));

        self.is_streaming = true;
        self.thinking_frame = 0;
        self.current_response.clear();
        let _ = self.message_tx.send(MessageCommand::SendMessage(message));
    }

    fn get_thinking_indicator(&self) -> &'static str {
        const BRAILLE_FRAMES: [&str; 8] = ["⠋", "⠙", "⠹", "⠸", "⠼", "⠴", "⠦", "⠧"];
        BRAILLE_FRAMES[self.thinking_frame % BRAILLE_FRAMES.len()]
    }

    fn advance_thinking_animation(&mut self) {
        self.thinking_frame = self.thinking_frame.wrapping_add(1);
    }

    fn check_message_responses(&mut self) {
        while let Ok(response) = self.message_rx.try_recv() {
            match response {
                MessageResponse::Chunk(chunk) => {
                    // Accumulate chunks in current_response
                    self.current_response.push_str(&chunk);
                }
                MessageResponse::Complete => {
                    self.is_streaming = false;
                    // Update cache from actual conversation
                    if let Ok(conv) = self.conversation.try_lock() {
                        self.cached_history = conv.history().to_vec();
                    }
                    self.current_response.clear();
                }
                MessageResponse::Error(err) => {
                    self.is_streaming = false;
                    self.status_message = Some(format!("Error: {}", err));
                    self.current_response.clear();
                }
            }
        }
    }

    async fn message_processor(
        conversation: Arc<Mutex<Conversation>>,
        mut cmd_rx: mpsc::UnboundedReceiver<MessageCommand>,
        resp_tx: mpsc::UnboundedSender<MessageResponse>,
    ) {
        while let Some(cmd) = cmd_rx.recv().await {
            match cmd {
                MessageCommand::SendMessage(message) => {
                    // Hold lock for entire operation (stream needs conversation reference)
                    let mut conv = conversation.lock().await;
                    match conv.send_stream(&message).await {
                        Ok(mut stream) => {
                            // Process stream and send chunks to UI immediately
                            while let Some(chunk) = stream.next().await {
                                // Send chunk notification to UI
                                let chunk_text = chunk.payload.content.iter()
                                    .filter_map(|c| {
                                        if let llm::api::ContentBlock::Text { text } = c {
                                            Some(text.clone())
                                        } else {
                                            None
                                        }
                                    })
                                    .collect::<String>();

                                let _ = resp_tx.send(MessageResponse::Chunk(chunk_text));
                            }
                            // Stream is dropped here, which updates conversation history
                            // Lock is released when conv goes out of scope
                            let _ = resp_tx.send(MessageResponse::Complete);
                        }
                        Err(e) => {
                            let _ = resp_tx.send(MessageResponse::Error(e.to_string()));
                        }
                    }
                }
            }
        }
    }
}

fn ui(f: &mut Frame, app_with_commands: &AppWithCommands) {
    let app = &app_with_commands.app;
    let completion_state = &app_with_commands.completion_state;

    let chunks = Layout::default()
        .direction(Direction::Vertical)
        .constraints([
            Constraint::Min(1),      // Chat area
            Constraint::Length(3),   // Input area
            Constraint::Length(1),   // Status bar
        ])
        .split(f.area());

    // Render chat messages
    // Use cached history if streaming, otherwise try to get fresh history
    let history = if app.is_streaming {
        app.cached_history.clone()
    } else if let Ok(conversation) = app.conversation.try_lock() {
        conversation.history().to_vec()
    } else {
        app.cached_history.clone()
    };

    let mut messages: Vec<ListItem> = history.iter().map(|msg| {
            let role = match msg.role {
                llm::Role::User => "You",
                llm::Role::Assistant => &app.model_display_name,
                llm::Role::System => "System",
            };

            let style = match msg.role {
                llm::Role::User => Style::default().fg(Color::Cyan),
                llm::Role::Assistant => Style::default().fg(Color::Green),
                llm::Role::System => Style::default().fg(Color::Yellow),
            };

            // Parse markdown and render
            let content_lines: Vec<Line> = msg.get_text()
                .lines()
                .map(|line| {
                    if line.starts_with("```") {
                        Line::from(Span::styled(line.to_string(), Style::default().fg(Color::DarkGray)))
                    } else if line.starts_with("# ") {
                        Line::from(Span::styled(
                            line.to_string(),
                            Style::default().fg(Color::Yellow).add_modifier(Modifier::BOLD),
                        ))
                    } else if line.starts_with("## ") {
                        Line::from(Span::styled(line.to_string(), Style::default().fg(Color::Yellow)))
                    } else if line.starts_with("- ") || line.starts_with("* ") {
                        Line::from(Span::styled(line.to_string(), Style::default().fg(Color::Cyan)))
                    } else if line.starts_with("`") && line.ends_with("`") {
                        Line::from(Span::styled(line.to_string(), Style::default().fg(Color::Magenta)))
                    } else {
                        Line::from(line.to_string())
                    }
                })
                .collect();

            let mut text: Text<'_> = Text::default();
            text.lines.push(Line::from(Span::styled(
                format!("[{}]", role),
                style.add_modifier(Modifier::BOLD),
            )));
            text.lines.extend(content_lines);
            text.lines.push(Line::from("".to_string()));

            ListItem::new(text)
        })
        .collect();

    // Add current streaming response if active
    if app.is_streaming && !app.current_response.is_empty() {
        let content_lines: Vec<Line> = app.current_response
            .lines()
            .map(|line| Line::from(line.to_string()))
            .collect();

        let mut text: Text<'_> = Text::default();
        text.lines.push(Line::from(Span::styled(
            format!("[{}]", app.model_display_name),
            Style::default().fg(Color::Green).add_modifier(Modifier::BOLD),
        )));
        text.lines.extend(content_lines);
        text.lines.push(Line::from("".to_string()));

        messages.push(ListItem::new(text));
    }

    let messages_list = List::new(messages)
        .block(Block::default().borders(Borders::ALL).title("Chat"));

    f.render_widget(messages_list, chunks[0]);

    // Render input box
    let input_widget = Paragraph::new(app.input.value())
        .style(Style::default().fg(Color::White))
        .block(Block::default().borders(Borders::ALL).title("Message (/ for commands)"));

    f.render_widget(input_widget, chunks[1]);

    // Render status bar
    let message_count = app.conversation.try_lock().map(|c| c.message_count()).unwrap_or(0);
    let status_text = if let Some(ref msg) = app.status_message {
        format!(" {} • {} | {} ", app.current_provider, app.model_display_name, msg)
    } else if matches!(completion_state, CompletionState::Loading) {
        format!(" {} • {} | {} Loading completions... ", app.current_provider, app.model_display_name, app.get_thinking_indicator())
    } else if app.is_streaming {
        format!(" {} • {} | {} Thinking... ", app.current_provider, app.model_display_name, app.get_thinking_indicator())
    } else {
        format!(" {} • {} | {} messages ", app.current_provider, app.model_display_name, message_count)
    };

    let status_bar = Paragraph::new(status_text)
        .style(Style::default().bg(Color::DarkGray).fg(Color::White));

    f.render_widget(status_bar, chunks[2]);

    // Render completion popup if showing
    if let CompletionState::Showing { completions, selected } = completion_state {
        let completion_items: Vec<ListItem> = completions
            .iter()
            .enumerate()
            .map(|(i, c)| {
                let label = c.label.as_ref().unwrap_or(&c.value);
                let desc = c.description.as_ref().map(|d| format!(" - {}", d)).unwrap_or_default();
                let text = format!("{}{}", label, desc);

                let style = if i == *selected {
                    Style::default().bg(Color::Blue).fg(Color::White)
                } else {
                    Style::default()
                };

                ListItem::new(text).style(style)
            })
            .collect();

        let completion_list = List::new(completion_items)
            .block(Block::default().borders(Borders::ALL).title("Completions"));

        // Position popup above input box
        let popup_height = (completions.len() as u16 + 2).min(10);
        let popup_y = chunks[1].y.saturating_sub(popup_height);
        let popup_area = ratatui::layout::Rect {
            x: chunks[1].x,
            y: popup_y,
            width: chunks[1].width,
            height: popup_height,
        };

        f.render_widget(completion_list, popup_area);
    }

    // Set cursor position in input box
    f.set_cursor_position((
        chunks[1].x + app.input.visual_cursor() as u16 + 1,
        chunks[1].y + 1,
    ));
}

#[tokio::main]
async fn main() -> Result<()> {
    // Setup terminal
    enable_raw_mode()?;
    let mut stdout = io::stdout();
    execute!(stdout, EnterAlternateScreen)?;
    let backend = CrosstermBackend::new(stdout);
    let mut terminal = Terminal::new(backend)?;

    // Create app and start
    let mut app = AppWithCommands::new(ModelProviderType::Gemini, None)?;
    app.start();

    let mut should_quit = false;

    while !should_quit {
        // Render UI
        terminal.draw(|f| ui(f, &app))?;

        // Update app state
        app.app.check_message_responses();
        if app.app.is_streaming {
            app.app.advance_thinking_animation();
        }

        // Handle input
        if event::poll(std::time::Duration::from_millis(100))? {
            if let Event::Key(key) = event::read()? {
                should_quit = !app.handle_key_event(key).await?;
            }
        }
    }

    // Restore terminal
    disable_raw_mode()?;
    execute!(terminal.backend_mut(), LeaveAlternateScreen)?;
    terminal.show_cursor()?;

    Ok(())
}
